epochs: 5
batch_size: 64
learning_rate:
  start_learning_rate: 0.0001
  peak_learning_rate: 0.001
  warmup_steps: 2
  reduction_factor: 0.5
  patience: 10
optimizer:
  weight_decay: 1.0e-5

